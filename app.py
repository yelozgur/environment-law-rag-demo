import os
import streamlit as st
import numpy as np
import fitz
import requests
from groq import Groq
from dotenv import load_dotenv
import faiss
import tempfile
import time
from pathlib import Path
import json

# Environment variables yÃ¼kle
load_dotenv()

# Streamlit sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="Ã‡evre Hukuku UzmanÄ±",
    page_icon="âš–ï¸",
    layout="wide"
)

class StreamlitRAGSystem:
    def __init__(self):
        """RAG sistemini baÅŸlat"""
        # API ayarlarÄ±
        self.HF_API_URL = "https://api-inference.huggingface.co/pipeline/feature-extraction/intfloat/multilingual-e5-small"
        self.HF_TOKEN = os.getenv("HF_TOKEN")
        self.HF_HEADERS = {"Authorization": f"Bearer {self.HF_TOKEN}"}
        
        # Groq API
        self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        self.model_name = "llama3-8b-8192"
        
        # Dosya yollarÄ±
        self.pdf_path = "documents/cevre_yasasi.pdf"
        self.index_path = "vectorstore/index.faiss"
        self.chunks_path = "vectorstore/chunks.npy"
        self.metadata_path = "vectorstore/metadata.json"
        
        # KlasÃ¶rleri oluÅŸtur
        self._ensure_directories()
        
        # Index yÃ¼kle
        self._load_index()
    
    def _ensure_directories(self):
        """Gerekli klasÃ¶rleri oluÅŸtur"""
        Path("documents").mkdir(exist_ok=True)
        Path("vectorstore").mkdir(exist_ok=True)
    
    def _load_index(self):
        """FAISS index ve chunk'larÄ± yÃ¼kle"""
        try:
            if os.path.exists(self.index_path) and os.path.exists(self.chunks_path):
                with st.spinner("ğŸ“¦ FAISS index yÃ¼kleniyor..."):
                    self.index = faiss.read_index(self.index_path)
                    self.chunks = np.load(self.chunks_path, allow_pickle=True)
                    
                    # Metadata yÃ¼kle (varsa)
                    if os.path.exists(self.metadata_path):
                        with open(self.metadata_path, 'r', encoding='utf-8') as f:
                            self.metadata = json.load(f)
                    else:
                        self.metadata = {"source": self.pdf_path, "chunks_count": len(self.chunks)}
                    
                st.session_state.index_loaded = True
                st.session_state.chunks_count = len(self.chunks)
                return True
            else:
                st.session_state.index_loaded = False
                return False
        except Exception as e:
            st.error(f"Index yÃ¼kleme hatasÄ±: {e}")
            st.session_state.index_loaded = False
            return False
    
    def _embed_text(self, text):
        """Metin iÃ§in embedding oluÅŸtur"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.HF_API_URL,
                    headers=self.HF_HEADERS,
                    json={"inputs": text},
                    timeout=30
                )
                
                if response.status_code == 200:
                    embedding = np.array(response.json(), dtype=np.float32)
                    
                    # Embedding boyutunu kontrol et
                    if embedding.ndim == 1:
                        embedding = embedding.reshape(1, -1)
                    
                    return embedding
                elif response.status_code == 503:  # Model loading
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 5
                        time.sleep(wait_time)
                        continue
                
                response.raise_for_status()
                
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                else:
                    st.error(f"Embedding oluÅŸturma hatasÄ±: {e}")
                    return None
        
        return None
    
    def create_index_from_existing_pdf(self):
        """Mevcut PDF'den index oluÅŸtur"""
        if not os.path.exists(self.pdf_path):
            st.error(f"PDF dosyasÄ± bulunamadÄ±: {self.pdf_path}")
            return False
        
        try:
            with st.spinner("ğŸ“„ Mevcut PDF iÅŸleniyor..."):
                # PDF'den metin Ã§Ä±kar
                doc = fitz.open(self.pdf_path)
                chunks = []
                page_chunk_map = []
                
                for page_num, page in enumerate(doc, 1):
                    text = page.get_text().strip()
                    if text:
                        # SayfayÄ± parÃ§alara bÃ¶l
                        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                        for para in paragraphs:
                            if len(para) > 30:  # Ã‡ok kÄ±sa paragraflarÄ± atla
                                chunks.append(para)
                                page_chunk_map.append(page_num)
                
                doc.close()
                
                if not chunks:
                    st.error("PDF'den metin Ã§Ä±karÄ±lamadÄ±!")
                    return False
                
                st.info(f"âœ… {len(chunks)} metin parÃ§asÄ± Ã§Ä±karÄ±ldÄ±")
            
            # Embedding oluÅŸtur
            with st.spinner("ğŸ”¨ Embedding'ler oluÅŸturuluyor..."):
                embeddings = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, chunk in enumerate(chunks):
                    status_text.text(f"ParÃ§a {i+1}/{len(chunks)} iÅŸleniyor...")
                    emb = self._embed_text(chunk)
                    if emb is not None:
                        embeddings.append(emb)
                    
                    # Ä°lerleme Ã§ubuÄŸunu gÃ¼ncelle
                    progress_bar.progress((i + 1) / len(chunks))
                
                status_text.empty()
                
                if not embeddings:
                    st.error("Embedding oluÅŸturulamadÄ±!")
                    return False
                
                embeddings_array = np.vstack(embeddings)
            
            # FAISS index oluÅŸtur
            with st.spinner("ğŸ—ï¸ FAISS index oluÅŸturuluyor..."):
                dim = embeddings_array.shape[1]
                index = faiss.IndexFlatL2(dim)
                index.add(embeddings_array)
                
                # Kaydet
                faiss.write_index(index, self.index_path)
                np.save(self.chunks_path, np.array(chunks, dtype=object))
                
                # Metadata kaydet
                metadata = {
                    "source": self.pdf_path,
                    "chunks_count": len(chunks),
                    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "page_chunk_map": page_chunk_map
                }
                
                with open(self.metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # Session state'i gÃ¼ncelle
            self.index = index
            self.chunks = np.array(chunks, dtype=object)
            self.metadata = metadata
            st.session_state.index_loaded = True
            st.session_state.chunks_count = len(chunks)
            
            st.success(f"âœ… Index oluÅŸturuldu: {len(chunks)} parÃ§a")
            return True
            
        except Exception as e:
            st.error(f"Index oluÅŸturma hatasÄ±: {e}")
            return False
    
    def create_index_from_new_pdf(self, pdf_file):
        """Yeni PDF'den index oluÅŸtur"""
        try:
            with st.spinner("ğŸ“„ Yeni PDF iÅŸleniyor..."):
                # GeÃ§ici dosya oluÅŸtur ve kaydet
                with open(self.pdf_path, 'wb') as f:
                    f.write(pdf_file.getvalue())
                
                # PDF'den metin Ã§Ä±kar
                doc = fitz.open(self.pdf_path)
                chunks = []
                page_chunk_map = []
                
                for page_num, page in enumerate(doc, 1):
                    text = page.get_text().strip()
                    if text:
                        # SayfayÄ± parÃ§alara bÃ¶l
                        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                        for para in paragraphs:
                            if len(para) > 30:  # Ã‡ok kÄ±sa paragraflarÄ± atla
                                chunks.append(para)
                                page_chunk_map.append(page_num)
                
                doc.close()
                
                if not chunks:
                    st.error("PDF'den metin Ã§Ä±karÄ±lamadÄ±!")
                    return False
                
                st.info(f"âœ… {len(chunks)} metin parÃ§asÄ± Ã§Ä±karÄ±ldÄ±")
            
            # Embedding oluÅŸtur
            with st.spinner("ğŸ”¨ Embedding'ler oluÅŸturuluyor..."):
                embeddings = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, chunk in enumerate(chunks):
                    status_text.text(f"ParÃ§a {i+1}/{len(chunks)} iÅŸleniyor...")
                    emb = self._embed_text(chunk)
                    if emb is not None:
                        embeddings.append(emb)
                    
                    # Ä°lerleme Ã§ubuÄŸunu gÃ¼ncelle
                    progress_bar.progress((i + 1) / len(chunks))
                
                status_text.empty()
                
                if not embeddings:
                    st.error("Embedding oluÅŸturulamadÄ±!")
                    return False
                
                embeddings_array = np.vstack(embeddings)
            
            # FAISS index oluÅŸtur
            with st.spinner("ğŸ—ï¸ FAISS index oluÅŸturuluyor..."):
                dim = embeddings_array.shape[1]
                index = faiss.IndexFlatL2(dim)
                index.add(embeddings_array)
                
                # Kaydet
                faiss.write_index(index, self.index_path)
                np.save(self.chunks_path, np.array(chunks, dtype=object))
                
                # Metadata kaydet
                metadata = {
                    "source": self.pdf_path,
                    "filename": pdf_file.name,
                    "chunks_count": len(chunks),
                    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "page_chunk_map": page_chunk_map
                }
                
                with open(self.metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # Session state'i gÃ¼ncelle
            self.index = index
            self.chunks = np.array(chunks, dtype=object)
            self.metadata = metadata
            st.session_state.index_loaded = True
            st.session_state.chunks_count = len(chunks)
            
            st.success(f"âœ… Index oluÅŸturuldu: {len(chunks)} parÃ§a")
            return True
            
        except Exception as e:
            st.error(f"Index oluÅŸturma hatasÄ±: {e}")
            return False
    
    def search(self, query, k=5):
        """Index'te benzer parÃ§alarÄ± ara"""
        if not hasattr(self, 'index') or self.index is None:
            return []
        
        # Query embedding
        query_emb = self._embed_text(query)
        if query_emb is None:
            return []
        
        # Arama
        distances, indices = self.index.search(query_emb.reshape(1, -1), k)
        
        # SonuÃ§larÄ± formatla
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):  # GeÃ§erli index kontrolÃ¼
                # Sayfa numarasÄ±nÄ± bul (varsa)
                page_num = self.metadata.get("page_chunk_map", [])[idx] if "page_chunk_map" in self.metadata else None
                
                results.append({
                    'text': self.chunks[idx],
                    'distance': float(distances[0][i]),
                    'similarity': 1 / (1 + distances[0][i]),  # Benzerlik skoru
                    'page': page_num,
                    'chunk_id': int(idx)
                })
        
        return results
    
    def ask_question(self, query, k=5):
        """Soru sor ve yanÄ±t al"""
        # Index kontrolÃ¼
        if not st.session_state.get('index_loaded', False):
            return {
                "answer": "LÃ¼tfen Ã¶nce bir PDF yÃ¼kleyin ve index oluÅŸturun.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Benzer parÃ§alarÄ± ara
        with st.spinner("ğŸ” Ä°lgili dokÃ¼manlar aranÄ±yor..."):
            results = self.search(query, k)
        
        if not results:
            return {
                "answer": "ÃœzgÃ¼nÃ¼m, ilgili dokÃ¼man bulunamadÄ±.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Context oluÅŸtur
        context_parts = []
        for i, result in enumerate(results):
            page_info = f" [Sayfa {result['page']}]" if result['page'] else ""
            context_parts.append(f"[ParÃ§a {i+1}{page_info}] {result['text']}")
        
        context = "\n\n---\n\n".join(context_parts)
        
        # Ortalama benzerlik
        avg_similarity = np.mean([r['similarity'] for r in results])
        
        # Prompt oluÅŸtur
        prompt = f"""Sen bir Ã§evre hukuku uzmanÄ± avukatsÄ±n.

KullanÄ±cÄ± Sorusu: {query}

Ä°lgili Mevzuat ParÃ§alarÄ±:
{context}

Ã–nemli Kurallar:
1. SADECE yukarÄ±daki mevzuat parÃ§alarÄ±na dayan
2. Belgede olmayan bilgi EKLEME
3. AnlaÅŸÄ±lÄ±r, profesyonel hukuki TÃ¼rkÃ§e kullan
4. Sayfa numaralarÄ±na referans ver
5. EÄŸer konu net deÄŸilse, "Belgede bu konu net belirtilmemiÅŸtir" de

YanÄ±t:"""
        
        # Groq API ile yanÄ±t al
        try:
            response = self.groq_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system", 
                        "content": "Sen bir Ã§evre hukuku uzmanÄ± avukatsÄ±n. Sadece verilen kaynaklara dayanarak cevap ver."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1500
            )
            
            answer = response.choices[0].message.content
            
            return {
                "answer": answer,
                "sources": results,
                "confidence": avg_similarity,
                "query": query
            }
            
        except Exception as e:
            st.error(f"API hatasÄ±: {e}")
            return {
                "answer": f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }

def main():
    """Ana Streamlit uygulamasÄ±"""
    st.title("âš–ï¸ Ã‡evre Hukuku Uzman AsistanÄ±")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("ğŸ“‚ DokÃ¼man YÃ¶netimi")
        
        # PDF dosya durumu
        st.subheader("ğŸ“„ Mevcut PDF")
        pdf_exists = os.path.exists("documents/cevre_yasasi.pdf")
        
        if pdf_exists:
            file_size = os.path.getsize("documents/cevre_yasasi.pdf") / 1024 / 1024
            st.success(f"âœ… cevre_yasasi.pdf ({file_size:.2f} MB)")
            
            if st.button("ğŸ”„ Mevcut PDF'den Index OluÅŸtur", type="primary"):
                if 'rag_system' not in st.session_state:
                    st.session_state.rag_system = StreamlitRAGSystem()
                
                rag = st.session_state.rag_system
                success = rag.create_index_from_existing_pdf()
                if success:
                    st.success("âœ… Index baÅŸarÄ±yla oluÅŸturuldu!")
                    time.sleep(2)
                    st.rerun()
        else:
            st.warning("âš ï¸ PDF bulunamadÄ±")
        
        st.markdown("---")
        st.subheader("ğŸ“¤ Yeni PDF YÃ¼kle")
        
        uploaded_file = st.file_uploader(
            "Yeni PDF yÃ¼kleyin",
            type=['pdf'],
            help="Mevcut PDF Ã¼zerine yazÄ±lacak"
        )
        
        if uploaded_file is not None:
            if st.button("ğŸ“¥ Yeni PDF ile Index OluÅŸtur", type="secondary"):
                if 'rag_system' not in st.session_state:
                    st.session_state.rag_system = StreamlitRAGSystem()
                
                rag = st.session_state.rag_system
                success = rag.create_index_from_new_pdf(uploaded_file)
                if success:
                    st.success("âœ… Yeni PDF ile index oluÅŸturuldu!")
                    time.sleep(2)
                    st.rerun()
        
        st.markdown("---")
        st.header("âš™ï¸ Ayarlar")
        
        k_results = st.slider(
            "Aranacak benzer dokÃ¼man sayÄ±sÄ±",
            min_value=1,
            max_value=10,
            value=5
        )
        
        st.markdown("---")
        st.subheader("ğŸ—„ï¸ Vector Store Durumu")
        
        # Index durumu
        index_exists = os.path.exists("vectorstore/index.faiss")
        chunks_exists = os.path.exists("vectorstore/chunks.npy")
        
        if index_exists and chunks_exists:
            st.success("âœ… Index yÃ¼klÃ¼")
            try:
                chunks = np.load("vectorstore/chunks.npy", allow_pickle=True)
                st.info(f"ğŸ“Š {len(chunks)} parÃ§a")
                
                # Metadata gÃ¶ster
                if os.path.exists("vectorstore/metadata.json"):
                    with open("vectorstore/metadata.json", 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    st.caption(f"Kaynak: {os.path.basename(metadata.get('source', 'Unknown'))}")
                    st.caption(f"OluÅŸturulma: {metadata.get('created_at', 'Unknown')}")
            except:
                st.info("ğŸ“Š Vector store mevcut")
        else:
            st.warning("âš ï¸ Index bulunamadÄ±")
        
        # Temizleme butonu
        st.markdown("---")
        if st.button("ğŸ—‘ï¸ Vector Store'u Temizle", type="secondary"):
            try:
                for file in ["vectorstore/index.faiss", "vectorstore/chunks.npy", "vectorstore/metadata.json"]:
                    if os.path.exists(file):
                        os.remove(file)
                st.success("âœ… Vector store temizlendi!")
                time.sleep(2)
                st.rerun()
            except Exception as e:
                st.error(f"Temizleme hatasÄ±: {e}")
    
    # Ana iÃ§erik alanÄ±
    # RAG sistemini baÅŸlat
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = StreamlitRAGSystem()
    
    rag = st.session_state.rag_system
    
    # Index durumunu kontrol et
    index_loaded = st.session_state.get('index_loaded', False)
    
    if not index_loaded:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.warning("""
            ### âš ï¸ Vector Store YÃ¼klenemedi
            
            **Mevcut Durum:**
            - PDF: `documents/cevre_yasasi.pdf` - {'âœ… Mevcut' if pdf_exists else 'âŒ Eksik'}
            - Index: `vectorstore/index.faiss` - {'âœ… Mevcut' if index_exists else 'âŒ Eksik'}
            - Chunks: `vectorstore/chunks.npy` - {'âœ… Mevcut' if chunks_exists else 'âŒ Eksik'}
            
            **Ne yapabilirsiniz:**
            1. **Mevcut PDF'den index oluÅŸtur** â†’ Sidebar'daki butonu kullanÄ±n
            2. **Yeni PDF yÃ¼kle** â†’ Sidebar'dan yeni PDF yÃ¼kleyin
            3. **Manuel kontrol** â†’ DosyalarÄ±n doÄŸru yerde olduÄŸundan emin olun
            """)
        
        with col2:
            st.info("""
            **ğŸ“ Dosya YapÄ±sÄ±:**
            ```
            main/
            â”œâ”€â”€ documents/
            â”‚   â””â”€â”€ cevre_yasasi.pdf
            â”œâ”€â”€ vectorstore/
            â”‚   â”œâ”€â”€ index.faiss
            â”‚   â”œâ”€â”€ chunks.npy
            â”‚   â””â”€â”€ metadata.json
            â”œâ”€â”€ app.py
            â””â”€â”€ requirements.txt
            ```
            """)
    
    # Soru sorma bÃ¶lÃ¼mÃ¼
    st.subheader("â“ Soru Sor")
    
    query = st.text_area(
        "Ã‡evre hukuku ile ilgili sorunuzu yazÄ±n:",
        placeholder="Ã–rnek: Ã‡evre kirliliÄŸi iÃ§in cezai yaptÄ±rÄ±mlar nelerdir? AtÄ±k yÃ¶netimi yÃ¼kÃ¼mlÃ¼lÃ¼kleri nelerdir? Ã‡evre izinleri nasÄ±l alÄ±nÄ±r?",
        height=100,
        disabled=not index_loaded
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if st.button("ğŸ” YanÄ±t Al", type="primary", disabled=not index_loaded, use_container_width=True) and query:
            if not index_loaded:
                st.error("LÃ¼tfen Ã¶nce vector store oluÅŸturun!")
                return
            
            # YanÄ±tÄ± al
            result = rag.ask_question(query, k=k_results)
            
            # YanÄ±tÄ± gÃ¶ster
            st.markdown("---")
            st.subheader("ğŸ¤– Uzman YanÄ±tÄ±")
            
            with st.container():
                st.markdown(result["answer"])
                
                # Ä°statistikler
                cols = st.columns(3)
                with cols[0]:
                    st.metric("GÃ¼ven Skoru", f"{result['confidence']:.2%}")
                with cols[1]:
                    st.metric("KullanÄ±lan Kaynak", len(result["sources"]))
                with cols[2]:
                    avg_page = np.mean([s.get('page', 0) for s in result['sources'] if s.get('page')])
                    if avg_page > 0:
                        st.metric("Ort. Sayfa No", f"{avg_page:.0f}")
            
            # KaynaklarÄ± gÃ¶ster
            if result["sources"]:
                with st.expander(f"ğŸ“š KullanÄ±lan Kaynaklar ({len(result['sources'])})", expanded=False):
                    for i, source in enumerate(result["sources"], 1):
                        st.markdown(f"**Kaynak {i}**")
                        
                        col_a, col_b = st.columns([1, 4])
                        with col_a:
                            st.metric("Benzerlik", f"{source['similarity']:.2%}")
                            if source.get('page'):
                                st.caption(f"Sayfa: {source['page']}")
                        
                        with col_b:
                            st.info(f"{source['text'][:500]}...")
                        
                        st.markdown("---")
    
    with col2:
        if st.button("ğŸ”„ SayfayÄ± Yenile", type="secondary", use_container_width=True):
            st.rerun()
    
    # Footer
    st.markdown("---")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.caption("âš¡ Powered by Groq API")
    with col2:
        st.caption("ğŸ” FAISS Vector Search")
    with col3:
        st.caption("âš–ï¸ Ã‡evre Hukuku UzmanÄ±")

if __name__ == "__main__":
    # Environment variables kontrolÃ¼
    groq_key = os.getenv("GROQ_API_KEY")
    hf_token = os.getenv("HF_TOKEN")
    
    if not groq_key:
        st.error("""
        ### âš ï¸ GROQ_API_KEY ayarlanmamÄ±ÅŸ!
        
        **Ã‡Ã¶zÃ¼m yollarÄ±:**
        
        1. **Streamlit Cloud Secrets:**
           ```toml
           # .streamlit/secrets.toml
           GROQ_API_KEY = "sk-..."
           HF_TOKEN = "hf_..."
           ```
        
        2. **Local .env dosyasÄ±:**
           ```bash
           # .env dosyasÄ± oluÅŸturun
           GROQ_API_KEY=sk-...
           HF_TOKEN=hf_...
           ```
        
        3. **Manuel giriÅŸ (geliÅŸtirme iÃ§in):**
        """)
        
        # GeliÅŸtirme iÃ§in manuel giriÅŸ
        with st.form("api_keys_form"):
            groq_input = st.text_input("GROQ API Key:", type="password")
            hf_input = st.text_input("HuggingFace Token:", type="password")
            
            if st.form_submit_button("API Key'leri Kaydet"):
                os.environ["GROQ_API_KEY"] = groq_input
                os.environ["HF_TOKEN"] = hf_input
                st.success("API Key'ler kaydedildi! SayfayÄ± yenileyin.")
                st.rerun()
    else:
        main()
