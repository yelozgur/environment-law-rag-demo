import os
import streamlit as st
import numpy as np
import fitz
import requests
import json
import tempfile
import time
from pathlib import Path
from groq import Groq
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# Environment variables y√ºkle
load_dotenv()

# Streamlit sayfa yapƒ±landƒ±rmasƒ±
st.set_page_config(
    page_title="√áevre Hukuku Uzmanƒ±",
    page_icon="‚öñÔ∏è",
    layout="wide"
)

# Cache'lenmi≈ü fonksiyonlar
@st.cache_resource
def load_embedding_model():
    """Hafif embedding modelini y√ºkle"""
    try:
        # CPU dostu, k√º√ß√ºk model
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        return model
    except Exception as e:
        st.error(f"Embedding model y√ºklenemedi: {e}")
        return None

@st.cache_resource
def init_chroma_client():
    """ChromaDB client'ƒ±nƒ± ba≈ülat"""
    try:
        # Streamlit Cloud i√ßin persist dizini
        persist_dir = "./chroma_db"
        Path(persist_dir).mkdir(exist_ok=True)
        
        client = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(
                chroma_db_impl="duckdb+parquet",
                anonymized_telemetry=False
            )
        )
        return client
    except Exception as e:
        st.error(f"ChromaDB ba≈ülatma hatasƒ±: {e}")
        return None

class ChromaRAGSystem:
    def __init__(self):
        """ChromaDB tabanlƒ± RAG sistemi"""
        self.embedding_model = load_embedding_model()
        self.chroma_client = init_chroma_client()
        
        # Groq API
        self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        self.model_name = "llama3-8b-8192"
        
        # Koleksiyon adƒ±
        self.collection_name = "cevre_hukuku"
        
        # Dosya yollarƒ±
        self.pdf_path = "documents/cevre_yasasi.pdf"
        self.metadata_path = "vectorstore/metadata.json"
        
        # Klas√∂rleri olu≈ütur
        self._ensure_directories()
        
        # Koleksiyonu y√ºkle
        self._load_collection()
    
    def _ensure_directories(self):
        """Gerekli klas√∂rleri olu≈ütur"""
        Path("documents").mkdir(exist_ok=True)
        Path("vectorstore").mkdir(exist_ok=True)
    
    def _load_collection(self):
        """ChromaDB koleksiyonunu y√ºkle veya olu≈ütur"""
        try:
            if self.chroma_client is None:
                st.error("ChromaDB client ba≈ülatƒ±lamadƒ±!")
                st.session_state.index_loaded = False
                return
            
            # Koleksiyonlarƒ± listele
            collections = self.chroma_client.list_collections()
            collection_names = [col.name for col in collections]
            
            if self.collection_name in collection_names:
                self.collection = self.chroma_client.get_collection(self.collection_name)
                count = self.collection.count()
                st.session_state.index_loaded = True
                st.session_state.chunks_count = count
                
                # Metadata y√ºkle
                if os.path.exists(self.metadata_path):
                    with open(self.metadata_path, 'r', encoding='utf-8') as f:
                        self.metadata = json.load(f)
                else:
                    self.metadata = {"source": self.pdf_path, "chunks_count": count}
                
                return True
            else:
                st.session_state.index_loaded = False
                return False
                
        except Exception as e:
            st.warning(f"Koleksiyon y√ºklenemedi, yeni olu≈üturulacak: {e}")
            st.session_state.index_loaded = False
            return False
    
    def _create_collection(self):
        """Yeni koleksiyon olu≈ütur"""
        try:
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={"description": "√áevre Hukuku Dok√ºmanlarƒ±"}
            )
            return True
        except Exception as e:
            st.error(f"Koleksiyon olu≈üturma hatasƒ±: {e}")
            return False
    
    def extract_text_from_pdf(self, pdf_path=None, pdf_file=None):
        """PDF'den metin √ßƒ±kar"""
        chunks = []
        page_chunk_map = []
        
        try:
            if pdf_file:
                # Ge√ßici dosya olu≈ütur
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(pdf_file.getvalue())
                    tmp_path = tmp_file.name
                doc_path = tmp_path
                is_temp = True
            else:
                doc_path = pdf_path or self.pdf_path
                is_temp = False
            
            if not os.path.exists(doc_path):
                st.error(f"PDF dosyasƒ± bulunamadƒ±: {doc_path}")
                return []
            
            # PDF'den metin √ßƒ±kar
            doc = fitz.open(doc_path)
            
            for page_num, page in enumerate(doc, 1):
                text = page.get_text().strip()
                if text:
                    # Paragraflara b√∂l
                    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                    for para in paragraphs:
                        if 50 < len(para) < 2000:  # Boyut kontrol√º
                            chunks.append(para)
                            page_chunk_map.append(page_num)
            
            doc.close()
            
            # Ge√ßici dosyayƒ± temizle
            if is_temp:
                os.unlink(doc_path)
            
            if chunks:
                st.info(f"‚úÖ {len(chunks)} metin par√ßasƒ± √ßƒ±karƒ±ldƒ±")
            else:
                st.warning("PDF'den metin √ßƒ±karƒ±lamadƒ±!")
            
            return chunks, page_chunk_map
            
        except Exception as e:
            st.error(f"PDF i≈üleme hatasƒ±: {e}")
            return [], []
    
    def create_embeddings(self, texts):
        """Metinler i√ßin embedding olu≈ütur"""
        if self.embedding_model is None:
            st.error("Embedding model y√ºklenemedi!")
            return None
        
        try:
            embeddings = self.embedding_model.encode(
                texts,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            return embeddings
        except Exception as e:
            st.error(f"Embedding olu≈üturma hatasƒ±: {e}")
            return None
    
    def create_index_from_existing_pdf(self):
        """Mevcut PDF'den index olu≈ütur"""
        if not os.path.exists(self.pdf_path):
            st.error(f"PDF dosyasƒ± bulunamadƒ±: {self.pdf_path}")
            return False
        
        with st.spinner("üìÑ Mevcut PDF i≈üleniyor..."):
            chunks, page_chunk_map = self.extract_text_from_pdf(self.pdf_path)
        
        if not chunks:
            return False
        
        return self._add_to_collection(chunks, page_chunk_map, self.pdf_path)
    
    def create_index_from_new_pdf(self, pdf_file):
        """Yeni PDF'den index olu≈ütur"""
        try:
            # PDF'i kaydet
            with open(self.pdf_path, 'wb') as f:
                f.write(pdf_file.getvalue())
            
            with st.spinner("üìÑ Yeni PDF i≈üleniyor..."):
                chunks, page_chunk_map = self.extract_text_from_pdf(pdf_file=pdf_file)
            
            if not chunks:
                return False
            
            return self._add_to_collection(chunks, page_chunk_map, pdf_file.name)
            
        except Exception as e:
            st.error(f"PDF kaydetme hatasƒ±: {e}")
            return False
    
    def _add_to_collection(self, chunks, page_chunk_map, source_name):
        """Koleksiyona par√ßalarƒ± ekle"""
        try:
            # Embedding olu≈ütur
            with st.spinner("üî® Embedding'ler olu≈üturuluyor..."):
                embeddings = self.create_embeddings(chunks)
                
                if embeddings is None:
                    return False
            
            # Koleksiyon olu≈ütur veya temizle
            try:
                self.chroma_client.delete_collection(self.collection_name)
            except:
                pass  # Koleksiyon yoksa sorun deƒüil
            
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            
            # Batch halinde ekle
            batch_size = 100
            for i in range(0, len(chunks), batch_size):
                end_idx = min(i + batch_size, len(chunks))
                batch_chunks = chunks[i:end_idx]
                batch_embeddings = embeddings[i:end_idx]
                batch_pages = page_chunk_map[i:end_idx]
                
                # Metadata hazƒ±rla
                metadatas = [
                    {
                        "page": batch_pages[j],
                        "source": source_name,
                        "chunk_id": i + j
                    }
                    for j in range(len(batch_chunks))
                ]
                
                # ID'ler olu≈ütur
                ids = [f"chunk_{i+j}" for j in range(len(batch_chunks))]
                
                # Koleksiyona ekle
                self.collection.add(
                    embeddings=batch_embeddings.tolist(),
                    documents=batch_chunks,
                    metadatas=metadatas,
                    ids=ids
                )
            
            # Metadata kaydet
            metadata = {
                "source": source_name,
                "chunks_count": len(chunks),
                "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "page_chunk_map": page_chunk_map
            }
            
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # Session state g√ºncelle
            st.session_state.index_loaded = True
            st.session_state.chunks_count = len(chunks)
            self.metadata = metadata
            
            st.success(f"‚úÖ Vector store olu≈üturuldu: {len(chunks)} par√ßa")
            return True
            
        except Exception as e:
            st.error(f"Koleksiyona ekleme hatasƒ±: {e}")
            return False
    
    def search(self, query, k=5):
        """Benzer par√ßalarƒ± ara"""
        if not st.session_state.get('index_loaded', False):
            return []
        
        try:
            # Query embedding
            query_embedding = self.embedding_model.encode(
                query,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            
            # ChromaDB'de ara
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Sonu√ßlarƒ± formatla
            formatted_results = []
            if results['documents']:
                for i, doc in enumerate(results['documents'][0]):
                    distance = results['distances'][0][i]
                    similarity = 1 - distance  # Cosine benzerliƒüi
                    
                    formatted_results.append({
                        'text': doc,
                        'distance': float(distance),
                        'similarity': float(similarity),
                        'page': results['metadatas'][0][i].get('page', 0),
                        'chunk_id': results['metadatas'][0][i].get('chunk_id', 0),
                        'source': results['metadatas'][0][i].get('source', 'Unknown')
                    })
            
            return formatted_results
            
        except Exception as e:
            st.error(f"Arama hatasƒ±: {e}")
            return []
    
    def ask_question(self, query, k=5):
        """Soru sor ve yanƒ±t al"""
        if not st.session_state.get('index_loaded', False):
            return {
                "answer": "L√ºtfen √∂nce bir PDF y√ºkleyin ve vector store olu≈üturun.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Benzer par√ßalarƒ± ara
        with st.spinner("üîç ƒ∞lgili dok√ºmanlar aranƒ±yor..."):
            results = self.search(query, k)
        
        if not results:
            return {
                "answer": "√úzg√ºn√ºm, ilgili dok√ºman bulunamadƒ±.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Context olu≈ütur
        context_parts = []
        for i, result in enumerate(results):
            page_info = f" [Sayfa {result['page']}]" if result.get('page') else ""
            context_parts.append(f"[Par√ßa {i+1}{page_info}] {result['text']}")
        
        context = "\n\n---\n\n".join(context_parts)
        
        # Ortalama benzerlik
        avg_similarity = np.mean([r['similarity'] for r in results])
        
        # Prompt olu≈ütur
        prompt = f"""Sen bir √ßevre hukuku uzmanƒ± avukatsƒ±n.

Kullanƒ±cƒ± Sorusu: {query}

ƒ∞lgili Mevzuat Par√ßalarƒ±:
{context}

√ñnemli Kurallar:
1. SADECE yukarƒ±daki mevzuat par√ßalarƒ±na dayan
2. Belgede olmayan bilgi EKLEME
3. Anla≈üƒ±lƒ±r, profesyonel hukuki T√ºrk√ße kullan
4. Sayfa numaralarƒ±na referans ver
5. Eƒüer konu net deƒüilse, "Belgede bu konu net belirtilmemi≈ütir" de

Yanƒ±t:"""
        
        # Groq API ile yanƒ±t al
        try:
            response = self.groq_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system", 
                        "content": "Sen bir √ßevre hukuku uzmanƒ± avukatsƒ±n. Sadece verilen kaynaklara dayanarak cevap ver."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1500
            )
            
            answer = response.choices[0].message.content
            
            return {
                "answer": answer,
                "sources": results,
                "confidence": avg_similarity,
                "query": query
            }
            
        except Exception as e:
            st.error(f"API hatasƒ±: {e}")
            return {
                "answer": f"√úzg√ºn√ºm, bir hata olu≈ütu: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }

def main():
    """Ana Streamlit uygulamasƒ±"""
    st.title("‚öñÔ∏è √áevre Hukuku Uzman Asistanƒ±")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("üìÇ Dok√ºman Y√∂netimi")
        
        # PDF dosya durumu
        st.subheader("üìÑ Mevcut PDF")
        pdf_exists = os.path.exists("documents/cevre_yasasi.pdf")
        
        if pdf_exists:
            file_size = os.path.getsize("documents/cevre_yasasi.pdf") / 1024 / 1024
            st.success(f"‚úÖ cevre_yasasi.pdf ({file_size:.2f} MB)")
            
            if st.button("üîÑ Mevcut PDF'den Vector Store Olu≈ütur", type="primary", use_container_width=True):
                if 'rag_system' not in st.session_state:
                    st.session_state.rag_system = ChromaRAGSystem()
                
                rag = st.session_state.rag_system
                success = rag.create_index_from_existing_pdf()
                if success:
                    st.success("‚úÖ Vector store ba≈üarƒ±yla olu≈üturuldu!")
                    time.sleep(2)
                    st.rerun()
        else:
            st.warning("‚ö†Ô∏è PDF bulunamadƒ±")
        
        st.markdown("---")
        st.subheader("üì§ Yeni PDF Y√ºkle")
        
        uploaded_file = st.file_uploader(
            "Yeni PDF y√ºkleyin",
            type=['pdf'],
            help="Mevcut PDF √ºzerine yazƒ±lacak"
        )
        
        if uploaded_file is not None:
            if st.button("üì• Yeni PDF ile Vector Store Olu≈ütur", type="secondary", use_container_width=True):
                if 'rag_system' not in st.session_state:
                    st.session_state.rag_system = ChromaRAGSystem()
                
                rag = st.session_state.rag_system
                success = rag.create_index_from_new_pdf(uploaded_file)
                if success:
                    st.success("‚úÖ Yeni PDF ile vector store olu≈üturuldu!")
                    time.sleep(2)
                    st.rerun()
        
        st.markdown("---")
        st.header("‚öôÔ∏è Ayarlar")
        
        k_results = st.slider(
            "Aranacak benzer dok√ºman sayƒ±sƒ±",
            min_value=1,
            max_value=10,
            value=5
        )
        
        st.markdown("---")
        st.subheader("üóÑÔ∏è Vector Store Durumu")
        
        # Vector store durumu
        if 'rag_system' in st.session_state and st.session_state.get('index_loaded', False):
            st.success("‚úÖ Vector store y√ºkl√º")
            chunks_count = st.session_state.get('chunks_count', 0)
            st.info(f"üìä {chunks_count} par√ßa")
            
            # Metadata g√∂ster
            if os.path.exists("vectorstore/metadata.json"):
                try:
                    with open("vectorstore/metadata.json", 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    st.caption(f"Kaynak: {os.path.basename(metadata.get('source', 'Unknown'))}")
                    st.caption(f"Olu≈üturulma: {metadata.get('created_at', 'Unknown')}")
                except:
                    pass
        else:
            st.warning("‚ö†Ô∏è Vector store y√ºklenmedi")
        
        # Temizleme butonu
        st.markdown("---")
        if st.button("üóëÔ∏è Vector Store'u Temizle", type="secondary", use_container_width=True):
            try:
                # ChromaDB koleksiyonunu sil
                if 'rag_system' in st.session_state:
                    try:
                        st.session_state.rag_system.chroma_client.delete_collection(
                            st.session_state.rag_system.collection_name
                        )
                    except:
                        pass
                
                # Metadata dosyalarƒ±nƒ± sil
                for file in ["vectorstore/metadata.json"]:
                    if os.path.exists(file):
                        os.remove(file)
                
                # ChromaDB dizinini temizle
                import shutil
                if os.path.exists("./chroma_db"):
                    shutil.rmtree("./chroma_db")
                
                # Session state'i sƒ±fƒ±rla
                if 'rag_system' in st.session_state:
                    del st.session_state.rag_system
                st.session_state.index_loaded = False
                
                st.success("‚úÖ Vector store temizlendi!")
                time.sleep(2)
                st.rerun()
            except Exception as e:
                st.error(f"Temizleme hatasƒ±: {e}")
    
    # Ana i√ßerik alanƒ±
    # RAG sistemini ba≈ülat
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = ChromaRAGSystem()
    
    rag = st.session_state.rag_system
    
    # Index durumunu kontrol et
    index_loaded = st.session_state.get('index_loaded', False)
    
    if not index_loaded:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.warning("""
            ### ‚ö†Ô∏è Vector Store Y√ºklenmedi
            
            **Ne yapabilirsiniz:**
            1. **Mevcut PDF'den vector store olu≈ütur** ‚Üí Sidebar'daki butonu kullanƒ±n
            2. **Yeni PDF y√ºkle** ‚Üí Sidebar'dan yeni PDF y√ºkleyin
            
            **üìÅ Dosya Yapƒ±sƒ±:**
            ```
            main/
            ‚îú‚îÄ‚îÄ documents/
            ‚îÇ   ‚îî‚îÄ‚îÄ cevre_yasasi.pdf
            ‚îú‚îÄ‚îÄ vectorstore/
            ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
            ‚îú‚îÄ‚îÄ chroma_db/ (otomatik olu≈üur)
            ‚îú‚îÄ‚îÄ app.py
            ‚îî‚îÄ‚îÄ requirements.txt
            ```
            """)
        
        with col2:
            st.info("""
            **üéØ √ñzellikler:**
            - ‚úÖ Python 3.13.9 uyumlu
            - ‚úÖ FAISS gerekmez
            - ‚úÖ ChromaDB kullanƒ±r
            - ‚úÖ Local embedding
            - ‚úÖ Persist storage
            """)
    
    # Soru sorma b√∂l√ºm√º
    st.subheader("‚ùì Soru Sor")
    
    query = st.text_area(
        "√áevre hukuku ile ilgili sorunuzu yazƒ±n:",
        placeholder="√ñrnek: √áevre kirliliƒüi i√ßin cezai yaptƒ±rƒ±mlar nelerdir? Atƒ±k y√∂netimi y√ºk√ºml√ºl√ºkleri nelerdir?",
        height=100,
        disabled=not index_loaded
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if st.button("üîç Yanƒ±t Al", type="primary", disabled=not index_loaded, use_container_width=True) and query:
            if not index_loaded:
                st.error("L√ºtfen √∂nce vector store olu≈üturun!")
                return
            
            # Yanƒ±tƒ± al
            result = rag.ask_question(query, k=k_results)
            
            # Yanƒ±tƒ± g√∂ster
            st.markdown("---")
            st.subheader("ü§ñ Uzman Yanƒ±tƒ±")
            
            with st.container():
                st.markdown(result["answer"])
                
                # ƒ∞statistikler
                cols = st.columns(3)
                with cols[0]:
                    st.metric("G√ºven Skoru", f"{result['confidence']:.2%}")
                with cols[1]:
                    st.metric("Kullanƒ±lan Kaynak", len(result["sources"]))
                with cols[2]:
                    pages = [s.get('page', 0) for s in result['sources'] if s.get('page', 0) > 0]
                    if pages:
                        st.metric("Sayfa No", f"{pages[0]}")
            
            # Kaynaklarƒ± g√∂ster
            if result["sources"]:
                with st.expander(f"üìö Kullanƒ±lan Kaynaklar ({len(result['sources'])})", expanded=False):
                    for i, source in enumerate(result["sources"], 1):
                        st.markdown(f"**Kaynak {i}**")
                        
                        col_a, col_b = st.columns([1, 4])
                        with col_a:
                            st.metric("Benzerlik", f"{source['similarity']:.2%}")
                            if source.get('page'):
                                st.caption(f"Sayfa: {source['page']}")
                        
                        with col_b:
                            st.info(f"{source['text'][:500]}...")
                        
                        st.markdown("---")
    
    with col2:
        if st.button("üîÑ Sayfayƒ± Yenile", type="secondary", use_container_width=True):
            st.rerun()
    
    # Footer
    st.markdown("---")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.caption("‚ö° Powered by Groq API")
    with col2:
        st.caption("üîç ChromaDB Vector Search")
    with col3:
        st.caption("‚öñÔ∏è √áevre Hukuku Uzmanƒ±")

if __name__ == "__main__":
    # Environment variables kontrol√º
    groq_key = os.getenv("GROQ_API_KEY")
    
    if not groq_key:
        st.error("""
        ### ‚ö†Ô∏è GROQ_API_KEY ayarlanmamƒ±≈ü!
        
        **√á√∂z√ºm yollarƒ±:**
        
        1. **Streamlit Cloud Secrets:**
           ```toml
           # .streamlit/secrets.toml
           GROQ_API_KEY = "sk-..."
           ```
        
        2. **Local .env dosyasƒ±:**
           ```bash
           # .env dosyasƒ± olu≈üturun
           GROQ_API_KEY=sk-...
           ```
        """)
    else:
        main()
