import os
import streamlit as st
import numpy as np
import fitz
import requests
from groq import Groq
from dotenv import load_dotenv
import faiss
import tempfile

# Environment variables yÃ¼kle
load_dotenv()

# Streamlit sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="Ã‡evre Hukuku UzmanÄ±",
    page_icon="âš–ï¸",
    layout="wide"
)

class StreamlitRAGSystem:
    def __init__(self):
        """RAG sistemini baÅŸlat"""
        # API ayarlarÄ±
        self.HF_API_URL = "https://api-inference.huggingface.co/pipeline/feature-extraction/intfloat/multilingual-e5-small"
        self.HF_TOKEN = os.getenv("HF_TOKEN")
        self.HF_HEADERS = {"Authorization": f"Bearer {self.HF_TOKEN}"}
        
        # Groq API
        self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        self.model_name = "llama3-8b-8192"
        
        # FAISS index yollarÄ±
        self.index_path = "documents/index.faiss"
        self.chunks_path = "documents/chunks.npy"
        
        # Index yÃ¼kle
        self._load_index()
    
    def _load_index(self):
        """FAISS index ve chunk'larÄ± yÃ¼kle"""
        try:
            if os.path.exists(self.index_path) and os.path.exists(self.chunks_path):
                self.index = faiss.read_index(self.index_path)
                self.chunks = np.load(self.chunks_path, allow_pickle=True)
                st.session_state.index_loaded = True
                return True
            else:
                st.warning("FAISS index bulunamadÄ±. LÃ¼tfen Ã¶nce PDF yÃ¼kleyin.")
                st.session_state.index_loaded = False
                return False
        except Exception as e:
            st.error(f"Index yÃ¼kleme hatasÄ±: {e}")
            st.session_state.index_loaded = False
            return False
    
    def _embed_text(self, text):
        """Metin iÃ§in embedding oluÅŸtur"""
        try:
            response = requests.post(
                self.HF_API_URL,
                headers=self.HF_HEADERS,
                json={"inputs": text},
                timeout=30
            )
            response.raise_for_status()
            
            embedding = np.array(response.json(), dtype=np.float32)
            
            # Embedding boyutunu kontrol et
            if embedding.ndim == 1:
                embedding = embedding.reshape(1, -1)
            
            return embedding
            
        except Exception as e:
            st.error(f"Embedding oluÅŸturma hatasÄ±: {e}")
            return None
    
    def create_index_from_pdf(self, pdf_file):
        """PDF'den FAISS index oluÅŸtur"""
        try:
            with st.spinner("ğŸ“„ PDF iÅŸleniyor..."):
                # GeÃ§ici dosya oluÅŸtur
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(pdf_file.getvalue())
                    tmp_path = tmp_file.name
                
                # PDF'den metin Ã§Ä±kar
                doc = fitz.open(tmp_path)
                chunks = []
                
                for page_num, page in enumerate(doc, 1):
                    text = page.get_text().strip()
                    if text:
                        # SayfayÄ± parÃ§alara bÃ¶l
                        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                        for para in paragraphs:
                            if len(para) > 30:  # Ã‡ok kÄ±sa paragraflarÄ± atla
                                chunks.append(para)
                
                doc.close()
                os.unlink(tmp_path)  # GeÃ§ici dosyayÄ± temizle
                
                if not chunks:
                    st.error("PDF'den metin Ã§Ä±karÄ±lamadÄ±!")
                    return False
                
                st.info(f"âœ… {len(chunks)} metin parÃ§asÄ± Ã§Ä±karÄ±ldÄ±")
            
            # Embedding oluÅŸtur
            with st.spinner("ğŸ”¨ Embedding'ler oluÅŸturuluyor..."):
                embeddings = []
                progress_bar = st.progress(0)
                
                for i, chunk in enumerate(chunks):
                    emb = self._embed_text(chunk)
                    if emb is not None:
                        embeddings.append(emb)
                    
                    # Ä°lerleme Ã§ubuÄŸunu gÃ¼ncelle
                    progress_bar.progress((i + 1) / len(chunks))
                
                if not embeddings:
                    st.error("Embedding oluÅŸturulamadÄ±!")
                    return False
                
                embeddings_array = np.vstack(embeddings)
            
            # FAISS index oluÅŸtur
            with st.spinner("ğŸ—ï¸ FAISS index oluÅŸturuluyor..."):
                dim = embeddings_array.shape[1]
                index = faiss.IndexFlatL2(dim)
                index.add(embeddings_array)
                
                # Kaydet
                faiss.write_index(index, self.index_path)
                np.save(self.chunks_path, np.array(chunks, dtype=object))
            
            # Session state'i gÃ¼ncelle
            self.index = index
            self.chunks = np.array(chunks, dtype=object)
            st.session_state.index_loaded = True
            
            st.success(f"âœ… Index oluÅŸturuldu ve kaydedildi: {len(chunks)} parÃ§a")
            return True
            
        except Exception as e:
            st.error(f"Index oluÅŸturma hatasÄ±: {e}")
            return False
    
    def search(self, query, k=5):
        """Index'te benzer parÃ§alarÄ± ara"""
        if not hasattr(self, 'index') or self.index is None:
            return []
        
        # Query embedding
        query_emb = self._embed_text(query)
        if query_emb is None:
            return []
        
        # Arama
        distances, indices = self.index.search(query_emb.reshape(1, -1), k)
        
        # SonuÃ§larÄ± formatla
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):  # GeÃ§erli index kontrolÃ¼
                results.append({
                    'text': self.chunks[idx],
                    'distance': distances[0][i],
                    'similarity': 1 / (1 + distances[0][i])  # Benzerlik skoru
                })
        
        return results
    
    def ask_question(self, query, k=5):
        """Soru sor ve yanÄ±t al"""
        # Index kontrolÃ¼
        if not st.session_state.get('index_loaded', False):
            return {
                "answer": "LÃ¼tfen Ã¶nce bir PDF yÃ¼kleyin ve index oluÅŸturun.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Benzer parÃ§alarÄ± ara
        with st.spinner("ğŸ” Ä°lgili dokÃ¼manlar aranÄ±yor..."):
            results = self.search(query, k)
        
        if not results:
            return {
                "answer": "ÃœzgÃ¼nÃ¼m, ilgili dokÃ¼man bulunamadÄ±.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Context oluÅŸtur
        context = "\n\n---\n\n".join([
            f"[ParÃ§a {i+1}] {result['text']}" 
            for i, result in enumerate(results)
        ])
        
        # Ortalama benzerlik
        avg_similarity = np.mean([r['similarity'] for r in results])
        
        # Prompt oluÅŸtur
        prompt = f"""Sen bir Ã§evre hukuku uzmanÄ± avukatsÄ±n.

KullanÄ±cÄ± Sorusu: {query}

Ä°lgili Mevzuat ParÃ§alarÄ±:
{context}

Ã–nemli Kurallar:
1. SADECE yukarÄ±daki mevzuat parÃ§alarÄ±na dayan
2. Belgede olmayan bilgi EKLEME
3. AnlaÅŸÄ±lÄ±r, profesyonel hukuki TÃ¼rkÃ§e kullan
4. "Belgede bu konu net belirtilmemiÅŸtir" gibi aÃ§Ä±k ifadeler kullan

YanÄ±t:"""
        
        # Groq API ile yanÄ±t al
        try:
            response = self.groq_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system", 
                        "content": "Sen bir Ã§evre hukuku uzmanÄ± avukatsÄ±n. Sadece verilen kaynaklara dayanarak cevap ver."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1024
            )
            
            answer = response.choices[0].message.content
            
            return {
                "answer": answer,
                "sources": results,
                "confidence": avg_similarity,
                "query": query
            }
            
        except Exception as e:
            st.error(f"API hatasÄ±: {e}")
            return {
                "answer": f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }

def main():
    """Ana Streamlit uygulamasÄ±"""
    st.title("âš–ï¸ Ã‡evre Hukuku Uzman AsistanÄ±")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("ğŸ“‚ DokÃ¼man YÃ¶netimi")
        
        # PDF yÃ¼kleme
        uploaded_file = st.file_uploader(
            "PDF dosyasÄ± yÃ¼kleyin",
            type=['pdf'],
            help="Ã‡evre hukuku ile ilgili PDF yÃ¼kleyin"
        )
        
        if uploaded_file is not None:
            if st.button("ğŸ“¥ PDF'den Index OluÅŸtur", type="primary"):
                # RAG sistemini baÅŸlat
                if 'rag_system' not in st.session_state:
                    st.session_state.rag_system = StreamlitRAGSystem()
                
                rag = st.session_state.rag_system
                
                # Index oluÅŸtur
                success = rag.create_index_from_pdf(uploaded_file)
                if success:
                    st.success("âœ… Index baÅŸarÄ±yla oluÅŸturuldu!")
                    st.rerun()
        
        st.markdown("---")
        st.header("âš™ï¸ Ayarlar")
        
        k_results = st.slider(
            "Aranacak benzer dokÃ¼man sayÄ±sÄ±",
            min_value=1,
            max_value=10,
            value=3
        )
        
        st.markdown("---")
        st.markdown("### ğŸ“– Mevcut Index")
        
        # Mevcut index durumu
        index_exists = os.path.exists("documents/index.faiss")
        chunks_exists = os.path.exists("documents/chunks.npy")
        
        if index_exists and chunks_exists:
            st.success("âœ… Index yÃ¼klÃ¼")
            try:
                chunks = np.load("documents/chunks.npy", allow_pickle=True)
                st.info(f"ğŸ“Š {len(chunks)} parÃ§a mevcut")
            except:
                st.info("ğŸ“Š Index mevcut")
        else:
            st.warning("âš ï¸ Index bulunamadÄ±")
    
    # Ana iÃ§erik alanÄ±
    # RAG sistemini baÅŸlat
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = StreamlitRAGSystem()
    
    rag = st.session_state.rag_system
    
    # Index durumunu kontrol et
    index_loaded = st.session_state.get('index_loaded', False)
    
    if not index_loaded:
        st.warning("""
        âš ï¸ **Index YÃ¼klenmedi**
        
        LÃ¼tfen:
        1. Sidebar'dan PDF yÃ¼kleyin
        2. "PDF'den Index OluÅŸtur" butonuna tÄ±klayÄ±n
        3. Ä°ÅŸlemin tamamlanmasÄ±nÄ± bekleyin
        
        Veya mevcut `documents/` klasÃ¶rÃ¼ndeki index dosyalarÄ±nÄ± kontrol edin.
        """)
    
    # Soru sorma bÃ¶lÃ¼mÃ¼
    st.subheader("â“ Soru Sor")
    
    query = st.text_area(
        "Ã‡evre hukuku ile ilgili sorunuzu yazÄ±n:",
        placeholder="Ã–rnek: Ã‡evre kirliliÄŸi iÃ§in cezai yaptÄ±rÄ±mlar nelerdir?",
        height=100,
        disabled=not index_loaded
    )
    
    if st.button("ğŸ” YanÄ±t Al", type="primary", disabled=not index_loaded) and query:
        if not index_loaded:
            st.error("LÃ¼tfen Ã¶nce index oluÅŸturun veya yÃ¼kleyin!")
            return
        
        # YanÄ±tÄ± al
        result = rag.ask_question(query, k=k_results)
        
        # YanÄ±tÄ± gÃ¶ster
        st.markdown("---")
        st.subheader("ğŸ¤– Uzman YanÄ±tÄ±")
        
        with st.container():
            st.markdown(result["answer"])
            
            # Ä°statistikler
            col1, col2 = st.columns(2)
            with col1:
                st.metric("GÃ¼ven Skoru", f"{result['confidence']:.2%}")
            with col2:
                st.metric("KullanÄ±lan Kaynak", len(result["sources"]))
        
        # KaynaklarÄ± gÃ¶ster
        if result["sources"]:
            with st.expander("ğŸ“š KullanÄ±lan Kaynaklar"):
                for i, source in enumerate(result["sources"], 1):
                    st.markdown(f"**Kaynak {i}** (Benzerlik: {source['similarity']:.2%})")
                    st.info(f"{source['text'][:400]}...")
                    st.markdown("---")
    
    # Footer
    st.markdown("---")
    st.caption("âš¡ Powered by Groq & FAISS | âš–ï¸ Ã‡evre Hukuku Uzman Sistemi")

if __name__ == "__main__":
    # Environment variables kontrolÃ¼
    if not os.getenv("GROQ_API_KEY"):
        st.error("""
        âš ï¸ **GROQ_API_KEY ayarlanmamÄ±ÅŸ!**
        
        LÃ¼tfen aÅŸaÄŸÄ±dakilerden birini yapÄ±n:
        
        1. `.env` dosyasÄ± oluÅŸturun:
        ```
        GROQ_API_KEY=your_api_key_here
        HF_TOKEN=your_huggingface_token_here
        ```
        
        2. Streamlit Cloud'da secrets kullanÄ±n:
        ```
        [secrets]
        GROQ_API_KEY = "your_api_key"
        HF_TOKEN = "your_hf_token"
        ```
        """)
    else:
        main()
